{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifiers for 6N vs Repaired\n",
    "\n",
    "In this notebook, we train various classifiers to distinguish between 6N and Repaired categories. The models utilize the hyperparameters optimized in the `_optimisation.ipynb` notebook. The classifiers include Random Forest, SVC, XGBoost, Rocket, MLP, and HIVECOTEV2. We also calculate SHAP values for interpretability, where feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import sys\n",
    "import seaborn as sns\n",
    "sys.path.append('../src')\n",
    "import abrTools as at\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "from collections import Counter\n",
    "import pretty_confusion_matrix as pcm\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Set the acquisition sampling rate\n",
    "fs = 195000.0 / 2.0\n",
    "\n",
    "# Import the datetime module to get today's date\n",
    "from datetime import date\n",
    "\n",
    "# Define the folder to save results\n",
    "savefolder = os.path.join('..', 'results', str(date.today()))\n",
    "\n",
    "# Create the folder if it does not exist\n",
    "if not os.path.exists(savefolder):\n",
    "    os.makedirs(savefolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train all models - Currently hyperparameters are optimised for F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "from sktime.classification.hybrid import HIVECOTEV2\n",
    "from sklearn.feature_selection import f_classif,mutual_info_classif, SelectFpr, SelectPercentile\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the frequencies and suffices for the datasets\n",
    "frequencies = [[100],[100,3000,6000,12000,18000,24000,30000,36000,42000]]\n",
    "suffices =  ['Click','Global']\n",
    "results = []\n",
    "njobs = -1\n",
    "anovaPercentile= 10\n",
    "\n",
    "# Loop through each frequency set\n",
    "for i, freq in enumerate(frequencies):\n",
    "    print(freq)\n",
    "    \n",
    "    # Create classification dataset\n",
    "    X_train, X_test, y_train, y_test, dataVersion = at.createClassificationDataset(test_size=0.25, oversample=False, ages=[1,], frequencies=freq)\n",
    "    X = np.vstack([X_train, X_test])\n",
    "    y = np.hstack([y_train, y_test])\n",
    "\n",
    "    # Random Forest Classifier with ANOVA feature selection\n",
    "    print('Forest - anova')\n",
    "    anova_fs = SelectPercentile(f_classif, percentile=anovaPercentile)\n",
    "    forest_cl = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1, max_depth=None, max_features='sqrt', min_samples_leaf=1, min_samples_split=5, bootstrap=True)\n",
    "    forest_pip = make_pipeline(anova_fs, forest_cl)\n",
    "    \n",
    "    # Check if results already exist, if not, fit the model and save results\n",
    "    if not os.path.exists(os.path.join(savefolder, f'forest_kFoldCrossValidation_AnovaFS{anovaPercentile}percent'+suffices[i]+'.csv')):\n",
    "        res = at.fitClassificationModel(forest_pip, X_train, y_train, X_test=X_test, y_test=y_test, saveToWandb=False, modelName='Forest classifier', dataVersion=dataVersion, crossValidation=True, makePlot=False, calculatePValue=False, njobs=njobs, n_splits=5)\n",
    "        pd.DataFrame(res).to_csv(os.path.join(savefolder, f'forest_kFoldCrossValidation_AnovaFS{anovaPercentile}percent'+suffices[i]+'.csv'))\n",
    "        results.append(res)\n",
    "        forest_pip.fit(X_train, y_train)\n",
    "        dump(forest_pip, os.path.join(savefolder, f'forest_kFoldCrossValidation_AnovaFS{anovaPercentile}percent'+suffices[i]+'.joblib'))\n",
    "        pd.DataFrame({'y_test': y_test, 'y_predict': forest_pip.predict(X_test)}).to_csv(os.path.join(savefolder, f'forest_kFoldCrossValidation_AnovaFS{anovaPercentile}percent'+suffices[i]+'testResults.csv'))\n",
    "\n",
    "    # Support Vector Classifier with ANOVA feature selection\n",
    "    print('svc - anova')\n",
    "    anova_fs = SelectPercentile(f_classif, percentile=10)\n",
    "    svc_cl = SVC(probability=True, C=0.01, class_weight='balanced', degree=2, gamma=0.01, kernel='poly', shrinking=True)\n",
    "    svc_pip = make_pipeline(anova_fs, svc_cl)\n",
    "    \n",
    "    # Check if results already exist, if not, fit the model and save results\n",
    "    if not os.path.exists(os.path.join(savefolder, f'SVC_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.csv')):\n",
    "        res = at.fitClassificationModel(svc_pip, X_train, y_train, X_test=X_test, y_test=y_test, saveToWandb=False, modelName='SVC classifier', dataVersion=dataVersion, crossValidation=True, makePlot=False, calculatePValue=False, njobs=njobs, n_splits=5)\n",
    "        pd.DataFrame(res).to_csv(os.path.join(savefolder, f'SVC_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.csv'))\n",
    "        results.append(res)\n",
    "        svc_pip.fit(X_train, y_train)\n",
    "        dump(svc_pip, os.path.join(savefolder, f'SVC_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.joblib'))\n",
    "        pd.DataFrame({'y_test': y_test, 'y_predict': svc_pip.predict(X_test)}).to_csv(os.path.join(savefolder, f'SVC_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'testResults.csv'))\n",
    "\n",
    "    # XGBoost Classifier with ANOVA feature selection\n",
    "    print('xgboost - anova')\n",
    "    anova_fs = SelectPercentile(f_classif, percentile=10)\n",
    "    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "    reg = xgb.XGBClassifier(n_estimators=200, verbosity=0, n_jobs=-1, random_state=42, max_depth=3, sample_weight=sample_weights, colsample_bytree=0.8, learning_rate=0.1, subsample=0.6)\n",
    "    xg_pip = make_pipeline(anova_fs, reg)\n",
    "    \n",
    "    # Encode labels for XGBoost\n",
    "    y_train2 = y_train.copy()\n",
    "    y_train2[y_train=='6N'] = 0\n",
    "    y_train2[y_train=='Repaired'] = 1\n",
    "    y_test2 = y_test.copy()\n",
    "    y_test2[y_test=='6N'] = 0\n",
    "    y_test2[y_test=='Repaired'] = 1\n",
    "    y_train2 = y_train2.astype(int)\n",
    "    y_test2 = y_test2.astype(int)\n",
    "    \n",
    "    # Check if results already exist, if not, fit the model and save results\n",
    "    if not os.path.exists(os.path.join(savefolder, f'XGBOOST_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.csv')):\n",
    "        res = at.fitClassificationModel(xg_pip, X_train, y_train2, X_test=X_test, y_test=y_test2, saveToWandb=False, modelName='XGBOOST', dataVersion=dataVersion, crossValidation=True, makePlot=False, njobs=njobs, encode_labels=True, n_splits=5)\n",
    "        pd.DataFrame(res).to_csv(os.path.join(savefolder, f'XGBOOST_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.csv'))\n",
    "        results.append(res)\n",
    "        xg_pip.fit(X_train, y_train2)\n",
    "        dump(xg_pip, os.path.join(savefolder, f'XGBOOST_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.joblib'))\n",
    "        y_predict2 = xg_pip.predict(X_test)\n",
    "        y_predict = y_predict2.copy().astype(str)\n",
    "        y_predict[y_predict2 == 0] = '6N'\n",
    "        y_predict[y_predict2 == 1] = 'Repaired'\n",
    "        pd.DataFrame({'y_test': y_test, 'y_predict': y_predict}).to_csv(os.path.join(savefolder, f'XGBOOST_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'testResults.csv'))\n",
    "\n",
    "    # Rocket Classifier with ANOVA feature selection\n",
    "    print('rocket - anova')\n",
    "    anova_fs = SelectPercentile(f_classif, percentile=10)\n",
    "    rocket = RocketClassifier(num_kernels=5000, n_jobs=-1, max_dilations_per_kernel=16, n_features_per_kernel=2, use_multivariate='yes', random_state=42)\n",
    "    rocket_pip = make_pipeline(anova_fs, rocket)\n",
    "    \n",
    "    # Check if results already exist, if not, fit the model and save results\n",
    "    if not os.path.exists(os.path.join(savefolder, f'Rocket_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.csv')):\n",
    "        try:\n",
    "            res = at.fitClassificationModel(rocket_pip, X_train, y_train, X_test=X_test, y_test=y_test, saveToWandb=False, modelName='Rocket classifier', dataVersion=dataVersion, crossValidation=True, makePlot=False, calculatePValue=False, njobs=njobs, n_splits=5)\n",
    "        except ValueError:\n",
    "            res = {'accuracy': array([0, 0, 0]), 'roc_auc_score': array([0, 0, 0])}\n",
    "        pd.DataFrame(res).to_csv(os.path.join(savefolder, f'Rocket_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.csv'))\n",
    "        results.append(res)\n",
    "        rocket_pip.fit(X_train, y_train)\n",
    "        dump(rocket_pip, os.path.join(savefolder, f'Rocket_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.joblib'))\n",
    "        pd.DataFrame({'y_test': y_test, 'y_predict': rocket_pip.predict(X_test)}).to_csv(os.path.join(savefolder, f'Rocket_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'testResults.csv'))\n",
    "\n",
    "    # MLP Classifier with ANOVA feature selection\n",
    "    print('MLP - anova')\n",
    "    anova_fs = SelectPercentile(f_classif, percentile=10)\n",
    "    mlp = MLPClassifier(solver='lbfgs', random_state=42, early_stopping=True, activation='tanh', alpha=0.05, hidden_layer_sizes=(150,), learning_rate_init=0.001, max_iter=100)\n",
    "    mlp_pip = make_pipeline(anova_fs, mlp)\n",
    "    \n",
    "    # Check if results already exist, if not, fit the model and save results\n",
    "    if not os.path.exists(os.path.join(savefolder, f'MLP_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.csv')):\n",
    "        res = at.fitClassificationModel(mlp_pip, X_train, y_train, X_test=X_test, y_test=y_test, saveToWandb=False, modelName='Forest classifier', dataVersion=dataVersion, crossValidation=True, makePlot=False, calculatePValue=False, njobs=njobs, n_splits=5)\n",
    "        pd.DataFrame(res).to_csv(os.path.join(savefolder, f'MLP_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.csv'))\n",
    "        results.append(res)\n",
    "        mlp_pip.fit(X_train, y_train)\n",
    "        dump(mlp_pip, os.path.join(savefolder, f'MLP_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.joblib'))\n",
    "        pd.DataFrame({'y_test': y_test, 'y_predict': mlp_pip.predict(X_test)}).to_csv(os.path.join(savefolder, f'MLP_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'testResults.csv'))\n",
    "\n",
    "    # HIVE-COTE Classifier with ANOVA feature selection\n",
    "    print('hive cote - anova')\n",
    "    anova_fs = SelectPercentile(f_classif, percentile=10)\n",
    "    hc2 = HIVECOTEV2(n_jobs=-1, time_limit_in_minutes=0.2, random_state=42)\n",
    "    hc2_pip = make_pipeline(anova_fs, hc2)\n",
    "    \n",
    "    # Check if results already exist, if not, fit the model and save results\n",
    "    if not os.path.exists(os.path.join(savefolder, f'hivecote_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.csv')):\n",
    "        try:\n",
    "            res = at.fitClassificationModel(hc2_pip, X_train, y_train, X_test=X_test, y_test=y_test, saveToWandb=False, modelName='Forest classifier', dataVersion=dataVersion, crossValidation=True, makePlot=False, calculatePValue=False, njobs=njobs, n_splits=5)\n",
    "        except ValueError:\n",
    "            res = {'accuracy': array([0, 0, 0]), 'roc_auc_score': array([0, 0, 0])}\n",
    "        pd.DataFrame(res).to_csv(os.path.join(savefolder, f'hivecote_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.csv'))\n",
    "        results.append(res)\n",
    "        hc2_pip.fit(X_train, y_train)\n",
    "        dump(hc2_pip, os.path.join(savefolder, f'hivecote_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.joblib'))\n",
    "        pd.DataFrame({'y_test': y_test, 'y_predict': hc2_pip.predict(X_test)}).to_csv(os.path.join(savefolder, f'hivecote_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'testResults.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Shapley values for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation is not computationally possible for bigger, non-tree based models (HC, ROCKET and MLP and SVC) as it requires too much RAM. SVC is doable for click datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "from sktime.classification.hybrid import HIVECOTEV2\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif, SelectFpr, SelectPercentile\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from joblib import dump, load\n",
    "import shap\n",
    "\n",
    "# Define the folder to save results\n",
    "savefolder = '../results/2024-10-25-main-optimisedForF1Score'\n",
    "\n",
    "# Define the frequencies and suffices for the datasets\n",
    "frequencies = [[100], [100, 3000, 6000, 12000, 18000, 24000, 30000, 36000, 42000]]\n",
    "suffices = ['Click', 'Global']\n",
    "results = []\n",
    "njobs = -1\n",
    "\n",
    "# Loop through each frequency set\n",
    "for i, freq in enumerate(frequencies):\n",
    "    print(freq)\n",
    "    \n",
    "    # Create classification dataset\n",
    "    X_train, X_test, y_train, y_test, dataVersion = at.createClassificationDataset(test_size=0.25, oversample=False, ages=[1,], frequencies=freq)\n",
    "    X = np.vstack([X_train, X_test])\n",
    "    y = np.hstack([y_train, y_test])\n",
    "\n",
    "    # Loop through each ANOVA percentile\n",
    "    for anovaPercentile in [10]:  # Modify to use other ANOVA percentiles\n",
    "        # We use TreeExplainer for RandomForest and XGBoost, and KernelExplainer for all other models\n",
    "\n",
    "        # Random Forest\n",
    "        savefile = os.path.join(savefolder, f'forest_kFoldCrossValidation_AnovaFS{anovaPercentile}percent'+suffices[i]+'ShapCoeff.csv')\n",
    "        if not os.path.exists(savefile):\n",
    "            try:\n",
    "                # Load the pre-trained RandomForest model\n",
    "                model = load(os.path.join(savefolder, f'forest_kFoldCrossValidation_AnovaFS{anovaPercentile}percent'+suffices[i]+'.joblib'))\n",
    "                \n",
    "                # Transform the test and train datasets using the feature selector\n",
    "                X_test_reduced = model[0].transform(X_test)\n",
    "                X_train_reduced = model[0].transform(X_train)\n",
    "                \n",
    "                # Sample 100 instances from the training set for SHAP value calculation\n",
    "                X100 = shap.utils.sample(X_train_reduced, 100)\n",
    "                \n",
    "                # Initialize the SHAP TreeExplainer with the RandomForest model\n",
    "                ke = shap.TreeExplainer(model[1], X100, approximate=True)\n",
    "                \n",
    "                # Calculate SHAP values for the test set\n",
    "                sv = ke.shap_values(X_test_reduced)\n",
    "                \n",
    "                # Inverse transform the SHAP values to the original feature space\n",
    "                class1Coeff = model[0].inverse_transform(sv[:, :, 0])\n",
    "                class2Coeff = model[0].inverse_transform(sv[:, :, 1])\n",
    "                \n",
    "                # Save the SHAP values to a CSV file\n",
    "                pd.DataFrame(class1Coeff.T).to_csv(savefile)\n",
    "            except FileNotFoundError:\n",
    "                # Handle the case where the model file is not found\n",
    "                pass\n",
    "\n",
    "        # XGBoost\n",
    "        savefile = os.path.join(savefolder, f'XGBOOST_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'ShapCoeff.csv')\n",
    "        if not os.path.exists(savefile):\n",
    "            try:\n",
    "                #Same as above but for XGBoost.\n",
    "                model = load(os.path.join(savefolder, f'XGBOOST_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.joblib'))\n",
    "                X_test_reduced = model[0].transform(X_test)\n",
    "                X_train_reduced = model[0].transform(X_train)\n",
    "                X100 = shap.utils.sample(X_train_reduced, 100)\n",
    "                ke = shap.TreeExplainer(model[1], X100, approximate=True)\n",
    "                sv = ke.shap_values(X_test_reduced)\n",
    "                class1Coeff = model[0].inverse_transform(sv)\n",
    "                pd.DataFrame(class1Coeff.T).to_csv(savefile)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "        # Loop through other models \n",
    "        for modelname in ['SVC']:  # SVC requires too much memory for the global model (600 GB at the moment)\n",
    "            print(modelname)\n",
    "            savefile = os.path.join(savefolder, f'{modelname}_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'ShapCoeff.csv')\n",
    "            if not os.path.exists(savefile):\n",
    "                try:\n",
    "                    model = load(os.path.join(savefolder, f'{modelname}_kFoldCrossValidation_AnovaFS{anovaPercentile}percent_'+suffices[i]+'.joblib'))\n",
    "                    X_test_reduced = model[0].transform(X_test[:5, :])  # We test on a subsample of data\n",
    "                    X_train_reduced = model[0].transform(X_train)\n",
    "                    X100 = shap.utils.sample(X_train_reduced, 50)  # We subsample 50 samples to speed up\n",
    "                    ke = shap.KernelExplainer(model[1].predict_proba, X100, approximate=True, check_additivity=False) # We use KernelExplainer for SVC\n",
    "                    sv = ke.shap_values(X_test_reduced)\n",
    "                    if sv.ndim == 3:\n",
    "                        class1Coeff = model[0].inverse_transform(sv[:, :, 0])\n",
    "                    else:\n",
    "                        if len(sv) > 1:\n",
    "                            class1Coeff = model[0].inverse_transform(sv[0])\n",
    "                        else:\n",
    "                            class1Coeff = model[0].inverse_transform(sv)\n",
    "                    pd.DataFrame(class1Coeff.T).to_csv(savefile)\n",
    "                except FileNotFoundError:\n",
    "                    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abr-ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
